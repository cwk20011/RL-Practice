{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+gisBBg+BrnFFqRQzpCvh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cwk20011/RL-Practice/blob/main/DDPG_optim_robot_arm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import torch.nn.functional as F\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # 로봇 팔을 모델링한 클래스\n",
        "# # RobotArmModel 클래스는 간단한 로봇 팔 모델을 나타냅니다. move 메서드는 주어진 행동을 사용하여 로봇 팔의 각도를 업데이트합니다.\n",
        "# class RobotArmModel:\n",
        "#     def __init__(self):\n",
        "#         self.angle = 0.0\n",
        "\n",
        "#     def move(self, action):\n",
        "#         # 행동을 적용하여 각도를 업데이트\n",
        "#         self.angle += action\n",
        "#         # 각도를 0에서 360 사이로 제한\n",
        "#         self.angle = self.angle % 360\n",
        "#         return self.angle\n",
        "\n",
        "# # DDPG 에이전트 클래스\n",
        "# # DDPGAgent 클래스는 DDPG 알고리즘을 구현한 에이전트입니다. __init__ 메서드에서는 Actor 신경망을 정의합니다. 여기서는 단순한 2층 신경망이며, 출력은 -1과 1 사이로 스케일 조정된 값입니다. select_action 메서드는 주어진 상태에서 액션을 선택합니다.\n",
        "# class DDPGAgent:\n",
        "#     def __init__(self, state_dim, action_dim):\n",
        "#         self.actor_net = nn.Sequential(\n",
        "#             nn.Linear(state_dim, 64),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(64, action_dim),\n",
        "#             nn.Tanh()  # -1과 1 사이의 값으로 스케일 조정\n",
        "#         )\n",
        "#         self.actor_optimizer = optim.Adam(self.actor_net.parameters(), lr=1e-3)\n",
        "\n",
        "#     def select_action(self, state):\n",
        "#         state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "#         with torch.no_grad():\n",
        "#             action = self.actor_net(state).numpy().flatten()\n",
        "#         return action\n",
        "\n",
        "#     def train(self, state, action, reward, next_state, done):\n",
        "#         # DDPG에서는 크리틱 네트워크를 사용하여 업데이트하지만, 여기서는 단순화를 위해 생략함\n",
        "#         pass\n",
        "\n",
        "# # 환경 초기화\n",
        "# robot_arm = RobotArmModel()\n",
        "# # 로봇 팔 모델과 DDPG 에이전트를 초기화합니다.\n",
        "# state_dim = 1  # 로봇 팔 각도\n",
        "# action_dim = 1  # 각도의 변화량\n",
        "\n",
        "# agent = DDPGAgent(state_dim, action_dim)\n",
        "\n",
        "# # 학습 루프\n",
        "# # 학습 루프에서는 에피소드를 반복하고, 각 에피소드는 10 스텝으로 가정합니다. 각 스텝에서는 현재 상태에서 액션을 선택하고, 로봇 팔을 이동시킵니다. 보상은 180도에 가까워지도록 설정되어 있습니다. 현재는 크리틱 네트워크를 사용하여 학습을 진행하지 않고 있습니다.\n",
        "# for episode in range(100):\n",
        "#     state = np.array([robot_arm.angle])  # 초기 상태는 로봇 팔의 현재 각도\n",
        "#     episode_reward = 0\n",
        "\n",
        "#     for _ in range(10):  # 각 에피소드는 10 스텝으로 가정\n",
        "#         action = agent.select_action(state)\n",
        "#         next_state = np.array([robot_arm.move(action)])\n",
        "#         reward = -(next_state - np.array([180.0]))**2  # 목표는 180도에 가까워지도록 함\n",
        "#         episode_reward += reward\n",
        "\n",
        "#         # 강화 학습 에이전트에 샘플 추가 및 학습 생략\n",
        "#         agent.train(state, action, reward, next_state, False)\n",
        "\n",
        "#         state = next_state\n",
        "\n",
        "#     print(f\"Episode: {episode+1}, Reward: {episode_reward.mean()}\")\n",
        "\n",
        "# # 최종 로봇 팔 각도 시각화\n",
        "# print(f\"Final Robot Arm Angle: {robot_arm.angle}\")\n"
      ],
      "metadata": {
        "id": "COBvIPRGWgI7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import torch.nn.functional as F\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # 로봇 팔을 모델링한 클래스\n",
        "# class RobotArmModel:\n",
        "#     def __init__(self):\n",
        "#         self.angle = 0.0\n",
        "\n",
        "#     def move(self, action):\n",
        "#         # 주어진 행동을 사용하여 로봇 팔의 각도 업데이트\n",
        "#         self.angle += action\n",
        "#         self.angle = self.angle % 360\n",
        "#         return self.angle\n",
        "\n",
        "# # Actor 네트워크 클래스\n",
        "# class ActorNet(nn.Module):\n",
        "#     def __init__(self, state_dim, action_dim):\n",
        "#         super(ActorNet, self).__init__()\n",
        "#         self.fc1 = nn.Linear(state_dim, 64)\n",
        "#         self.fc2 = nn.Linear(64, action_dim)\n",
        "\n",
        "#     def forward(self, state):\n",
        "#         x = F.relu(self.fc1(state))\n",
        "#         x = torch.tanh(self.fc2(x))  # -1과 1 사이의 값으로 스케일 조정\n",
        "#         return x\n",
        "\n",
        "# # Critic 네트워크 클래스\n",
        "# class CriticNet(nn.Module):\n",
        "#     def __init__(self, state_dim, action_dim):\n",
        "#         super(CriticNet, self).__init__()\n",
        "#         self.fc_state = nn.Linear(state_dim, 32)\n",
        "#         self.fc_action = nn.Linear(action_dim, 32)\n",
        "#         self.fc2 = nn.Linear(64, 1)\n",
        "\n",
        "#     def forward(self, state, action):\n",
        "#         x_state = F.relu(self.fc_state(state))\n",
        "#         x_action = F.relu(self.fc_action(action))\n",
        "#         x = torch.cat((x_state, x_action), dim=1)\n",
        "#         x = F.relu(self.fc2(x))\n",
        "#         return x\n",
        "\n",
        "# # DDPG 에이전트 클래스\n",
        "# class DDPGAgent:\n",
        "#     def __init__(self, state_dim, action_dim):\n",
        "#         # Actor, Critic 네트워크 및 타겟 네트워크 초기화\n",
        "#         self.actor_net = ActorNet(state_dim, action_dim)\n",
        "#         self.critic_net = CriticNet(state_dim, action_dim)\n",
        "#         self.target_actor_net = ActorNet(state_dim, action_dim)\n",
        "#         self.target_critic_net = CriticNet(state_dim, action_dim)\n",
        "\n",
        "#         # Actor, Critic의 옵티마이저 초기화\n",
        "#         self.actor_optimizer = optim.Adam(self.actor_net.parameters(), lr=1e-3)\n",
        "#         self.critic_optimizer = optim.Adam(self.critic_net.parameters(), lr=1e-3)\n",
        "\n",
        "#         # Replay 메모리 초기화\n",
        "#         self.memory = Memory(2000)\n",
        "\n",
        "#         # 타겟 네트워크 업데이트 주기\n",
        "#         self.target_update_freq = 200\n",
        "#         self.target_update_counter = 0\n",
        "\n",
        "#     def select_action(self, state):\n",
        "#         # 주어진 상태에서 액션 선택\n",
        "#         state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "#         with torch.no_grad():\n",
        "#             action = self.actor_net(state).numpy().flatten()\n",
        "#         return action\n",
        "\n",
        "#     def train(self, state, action, reward, next_state, done):\n",
        "#         transitions = self.memory.sample(32)\n",
        "#         if not transitions:\n",
        "#             return  # 메모리에서 샘플이 부족하면 학습을 진행하지 않음\n",
        "\n",
        "#         batch = Transition(*zip(*transitions))\n",
        "\n",
        "#         # 타겟 Q 값 계산\n",
        "#         with torch.no_grad():\n",
        "#             target_actions = self.target_actor_net(batch.next_state)\n",
        "#             target_q_values = self.target_critic_net(batch.next_state, target_actions).squeeze()\n",
        "#             target_q_values = target_q_values * (1 - batch.done)\n",
        "#             target_q_values += batch.reward\n",
        "\n",
        "#         # Critic 네트워크 업데이트\n",
        "#         critic_loss = F.smooth_l1_loss(self.critic_net(batch.state, batch.action).squeeze(), target_q_values)\n",
        "#         self.critic_optimizer.zero_grad()\n",
        "#         critic_loss.backward()\n",
        "#         self.critic_optimizer.step()\n",
        "\n",
        "#         # Actor 네트워크 업데이트\n",
        "#         actor_loss = -self.critic_net(batch.state, self.actor_net(batch.state)).mean()\n",
        "#         self.actor_optimizer.zero_grad()\n",
        "#         actor_loss.backward()\n",
        "#         self.actor_optimizer.step()\n",
        "\n",
        "#         # 타겟 네트워크 주기적 업데이트\n",
        "#         if self.target_update_counter % self.target_update_freq == 0:\n",
        "#             self.target_critic_net.load_state_dict(self.critic_net.state_dict())\n",
        "#             self.target_actor_net.load_state_dict(self.actor_net.state_dict())\n",
        "\n",
        "#         self.target_update_counter += 1\n",
        "\n",
        "# # Replay 메모리 클래스\n",
        "# class Memory:\n",
        "#     def __init__(self, capacity):\n",
        "#         self.memory = []\n",
        "#         self.capacity = capacity\n",
        "#         self.data_pointer = 0\n",
        "\n",
        "#     def update(self, transition):\n",
        "#         if len(self.memory) < self.capacity:\n",
        "#             self.memory.append(transition)\n",
        "#         else:\n",
        "#             self.memory[self.data_pointer] = transition\n",
        "#         self.data_pointer = (self.data_pointer + 1) % self.capacity\n",
        "\n",
        "#     def sample(self, batch_size):\n",
        "#         if len(self.memory) < batch_size:\n",
        "#             return []\n",
        "#         return np.random.choice(self.memory, batch_size, replace=False)\n",
        "\n",
        "# # Transition 클래스 정의\n",
        "# class Transition:\n",
        "#     def __init__(self, state=None, action=None, reward=None, next_state=None, done=None):\n",
        "#         self.state = state\n",
        "#         self.action = action\n",
        "#         self.reward = reward\n",
        "#         self.next_state = next_state\n",
        "#         self.done = done\n",
        "\n",
        "# # 환경 초기화\n",
        "# robot_arm = RobotArmModel()\n",
        "# state_dim = 1  # 로봇 팔 각도\n",
        "# action_dim = 1  # 각도의 변화량\n",
        "\n",
        "# # DDPG 에이전트 초기화\n",
        "# agent = DDPGAgent(state_dim, action_dim)\n",
        "\n",
        "# # 학습 루프\n",
        "# for episode in range(3000):\n",
        "#     state = np.array([robot_arm.angle])\n",
        "#     episode_reward = 0\n",
        "\n",
        "#     for _ in range(20):\n",
        "#         action = agent.select_action(state)\n",
        "#         next_state = np.array([robot_arm.move(action)])\n",
        "#         reward = -(next_state - np.array([180.0]))**2\n",
        "#         episode_reward += reward\n",
        "\n",
        "#         agent.train(state, action, reward, next_state, False)\n",
        "\n",
        "#         state = next_state\n",
        "\n",
        "#     if (episode + 1) % 20 == 0:\n",
        "#         print(f\"Episode: {episode+1}, Reward: {episode_reward.mean()}\")\n",
        "\n",
        "# # 최종 로봇 팔 각도 시각화\n",
        "# print(f\"Final Robot Arm Angle: {robot_arm.angle}\")\n"
      ],
      "metadata": {
        "id": "g7s2_71Sipj-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 로봇 팔을 모델링한 클래스\n",
        "class RobotArmModel:\n",
        "    def __init__(self):\n",
        "        self.angle = 0.0\n",
        "\n",
        "    def move(self, action):\n",
        "        # 주어진 행동을 사용하여 로봇 팔의 각도 업데이트\n",
        "        self.angle += action\n",
        "        self.angle = self.angle % 360\n",
        "        return self.angle\n",
        "\n",
        "# Actor 네트워크 클래스\n",
        "class ActorNet(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(ActorNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = torch.tanh(self.fc2(x))  # -1과 1 사이의 값으로 스케일 조정\n",
        "        return x\n",
        "\n",
        "# Critic 네트워크 클래스\n",
        "class CriticNet(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(CriticNet, self).__init__()\n",
        "        self.fc_state = nn.Linear(state_dim, 64)  # 수정: 뉴런 수 증가\n",
        "        self.fc_action = nn.Linear(action_dim, 64)  # 수정: 뉴런 수 증가\n",
        "        self.fc2 = nn.Linear(128, 1)  # 수정: 입력 차원 변경\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x_state = F.relu(self.fc_state(state))\n",
        "        x_action = F.relu(self.fc_action(action))\n",
        "        x = torch.cat((x_state, x_action), dim=1)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# DDPG 에이전트 클래스\n",
        "class DDPGAgent:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        # Actor, Critic 네트워크 및 타겟 네트워크 초기화\n",
        "        self.actor_net = ActorNet(state_dim, action_dim)\n",
        "        self.critic_net = CriticNet(state_dim, action_dim)\n",
        "        self.target_actor_net = ActorNet(state_dim, action_dim)\n",
        "        self.target_critic_net = CriticNet(state_dim, action_dim)\n",
        "\n",
        "        # Actor, Critic의 옵티마이저 초기화\n",
        "        self.actor_optimizer = optim.Adam(self.actor_net.parameters(), lr=1e-4)  # 수정: 학습률 조정\n",
        "        self.critic_optimizer = optim.Adam(self.critic_net.parameters(), lr=1e-4)  # 수정: 학습률 조정\n",
        "\n",
        "        # Replay 메모리 초기화\n",
        "        self.memory = Memory(5000)  # 수정: 메모리 크기 증가\n",
        "\n",
        "        # 타겟 네트워크 업데이트 주기\n",
        "        self.target_update_freq = 160  # 수정: 업데이트 주기 감소\n",
        "        self.target_update_counter = 0\n",
        "\n",
        "        # 추가: 노이즈 설정\n",
        "        self.noise_std = 0.1\n",
        "\n",
        "    def select_action(self, state):\n",
        "        # 주어진 상태에서 액션 선택\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            action = self.actor_net(state).numpy().flatten()\n",
        "        # 추가: 액션에 노이즈 추가\n",
        "        action += np.random.normal(0, self.noise_std, action.shape)\n",
        "        return action\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done):\n",
        "        transitions = self.memory.sample(32)\n",
        "        if not transitions:\n",
        "            return  # 메모리에서 샘플이 부족하면 학습을 진행하지 않음\n",
        "\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "        # 타겟 Q 값 계산\n",
        "        with torch.no_grad():\n",
        "            target_actions = self.target_actor_net(batch.next_state)\n",
        "            target_q_values = self.target_critic_net(batch.next_state, target_actions).squeeze()\n",
        "            target_q_values = target_q_values * (1 - batch.done)\n",
        "            target_q_values += batch.reward\n",
        "\n",
        "        # Critic 네트워크 업데이트\n",
        "        critic_loss = F.smooth_l1_loss(self.critic_net(batch.state, batch.action).squeeze(), target_q_values)\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # Actor 네트워크 업데이트\n",
        "        actor_loss = -self.critic_net(batch.state, self.actor_net(batch.state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # 타겟 네트워크 주기적 업데이트\n",
        "        if self.target_update_counter % self.target_update_freq == 0:\n",
        "            self.target_critic_net.load_state_dict(self.critic_net.state_dict())\n",
        "            self.target_actor_net.load_state_dict(self.actor_net.state_dict())\n",
        "\n",
        "        self.target_update_counter += 1\n",
        "\n",
        "# Replay 메모리 클래스\n",
        "class Memory:\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = []\n",
        "        self.capacity = capacity\n",
        "        self.data_pointer = 0\n",
        "\n",
        "    def update(self, transition):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(transition)\n",
        "        else:\n",
        "            self.memory[self.data_pointer] = transition\n",
        "        self.data_pointer = (self.data_pointer + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return []\n",
        "        return np.random.choice(self.memory, batch_size, replace=False)\n",
        "\n",
        "# Transition 클래스 정의\n",
        "class Transition:\n",
        "    def __init__(self, state=None, action=None, reward=None, next_state=None, done=None):\n",
        "        self.state = state\n",
        "        self.action = action\n",
        "        self.reward = reward\n",
        "        self.next_state = next_state\n",
        "        self.done = done\n",
        "\n",
        "# 환경 초기화\n",
        "robot_arm = RobotArmModel()\n",
        "state_dim = 1  # 로봇 팔 각도\n",
        "action_dim = 1  # 각도의 변화량\n",
        "\n",
        "# DDPG 에이전트 초기화\n",
        "agent = DDPGAgent(state_dim, action_dim)\n",
        "\n",
        "# 학습 루프\n",
        "for episode in range(3000):\n",
        "    state = np.array([robot_arm.angle])\n",
        "    episode_reward = 0\n",
        "\n",
        "    for _ in range(20):\n",
        "        action = agent.select_action(state)\n",
        "        next_state = np.array([robot_arm.move(action)])\n",
        "        reward = -(next_state - np.array([180.0]))**2\n",
        "        episode_reward += reward\n",
        "\n",
        "        agent.train(state, action, reward, next_state, False)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    if (episode + 1) % 100 == 0:\n",
        "        print(f\"Episode: {episode+1}, Reward: {episode_reward.mean()}\")\n",
        "\n",
        "# 최종 로봇 팔 각도 시각화\n",
        "print(f\"Final Robot Arm Angle: {robot_arm.angle}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPKu-PGooBnQ",
        "outputId": "feffa958-94dd-480c-f10b-851970be9d3c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100, Reward: -644182.0013428832\n",
            "Episode: 200, Reward: -644684.6183650227\n",
            "Episode: 300, Reward: -644517.2740761095\n",
            "Episode: 400, Reward: -644652.6093819063\n",
            "Episode: 500, Reward: -642849.1788018813\n",
            "Episode: 600, Reward: -644071.136802209\n",
            "Episode: 700, Reward: -644840.2800157437\n",
            "Episode: 800, Reward: -644699.3720598648\n",
            "Episode: 900, Reward: -644489.8293210773\n",
            "Episode: 1000, Reward: -645317.4926343367\n",
            "Episode: 1100, Reward: -644716.2219138166\n",
            "Episode: 1200, Reward: -644485.9494466683\n",
            "Episode: 1300, Reward: -644211.4032754991\n",
            "Episode: 1400, Reward: -644281.6841103337\n",
            "Episode: 1500, Reward: -644215.7333778013\n",
            "Episode: 1600, Reward: -644494.284264935\n",
            "Episode: 1700, Reward: -644683.0083070691\n",
            "Episode: 1800, Reward: -643994.0040909263\n",
            "Episode: 1900, Reward: -644550.5472453091\n",
            "Episode: 2000, Reward: -644923.456712318\n",
            "Episode: 2100, Reward: -644985.2739900239\n",
            "Episode: 2200, Reward: -645100.2920768801\n",
            "Episode: 2300, Reward: -645096.2787066055\n",
            "Episode: 2400, Reward: -644111.9022459198\n",
            "Episode: 2500, Reward: -644245.7669293189\n",
            "Episode: 2600, Reward: -645414.4630052764\n",
            "Episode: 2700, Reward: -645047.5548335311\n",
            "Episode: 2800, Reward: -645088.9818576577\n",
            "Episode: 2900, Reward: -644306.2863272692\n",
            "Episode: 3000, Reward: -644276.6967238449\n",
            "Final Robot Arm Angle: [1.1204995]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zNUKaXqJrqCu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}