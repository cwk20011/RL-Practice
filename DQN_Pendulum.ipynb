{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090075c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import namedtuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# 학습 기록 저장을 위한 namedtuple 정의\n",
    "TrainingRecord = namedtuple('TrainingRecord', ['ep', 'reward'])\n",
    "Transition = namedtuple('Transition', ['s', 'a', 'r', 's_'])\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(3, 100)\n",
    "        self.a_head = nn.Linear(100, 5)\n",
    "        self.v_head = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc(x))\n",
    "        a = self.a_head(x) - self.a_head(x).mean(1, keepdim=True)\n",
    "        v = self.v_head(x)\n",
    "        action_scores = a + v\n",
    "        return action_scores\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = np.empty(capacity, dtype=object)\n",
    "        self.capacity = capacity\n",
    "        self.data_pointer = 0\n",
    "        self.isfull = False\n",
    "\n",
    "    def update(self, transition):\n",
    "        self.memory[self.data_pointer] = transition\n",
    "        self.data_pointer += 1\n",
    "        if self.data_pointer == self.capacity:\n",
    "            self.data_pointer = 0\n",
    "            self.isfull = True\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return np.random.choice(self.memory, batch_size)\n",
    "\n",
    "class DQNAgent():\n",
    "    action_list = [(i * 4 - 2,) for i in range(5)]\n",
    "    max_grad_norm = 0.5\n",
    "\n",
    "    def __init__(self):\n",
    "        self.training_step = 0\n",
    "        self.epsilon = 1\n",
    "        self.eval_net, self.target_net = QNetwork().float(), QNetwork().float()\n",
    "        self.memory = ReplayMemory(2000)\n",
    "        self.optimizer = optim.Adam(self.eval_net.parameters(), lr=1e-3)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action_index = np.random.randint(5)\n",
    "        else:\n",
    "            probs = self.eval_net(state)\n",
    "            action_index = probs.max(1)[1].item()\n",
    "        return self.action_list[action_index], action_index\n",
    "\n",
    "    def save_param(self):\n",
    "        torch.save(self.eval_net.state_dict(), './dqn_net_params.pkl')\n",
    "\n",
    "    def store_transition(self, transition):\n",
    "        self.memory.update(transition)\n",
    "\n",
    "    def update(self):\n",
    "        self.training_step += 1\n",
    "\n",
    "        transitions = self.memory.sample(32)\n",
    "        s = torch.tensor([t.s for t in transitions], dtype=torch.float)\n",
    "        a = torch.tensor([t.a for t in transitions], dtype=torch.long).view(-1, 1)\n",
    "        r = torch.tensor([t.r for t in transitions], dtype=torch.float).view(-1, 1)\n",
    "        s_ = torch.tensor([t.s_ for t in transitions], dtype=torch.float)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            a_ = self.eval_net(s_).max(1, keepdim=True)[1]\n",
    "            q_target = r + 0.9 * self.target_net(s_).gather(1, a_)\n",
    "        q_eval = self.eval_net(s).gather(1, a)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = F.smooth_l1_loss(q_eval, q_target)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.eval_net.parameters(), self.max_grad_norm)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.training_step % 200 == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "\n",
    "        self.epsilon = max(self.epsilon * 0.999, 0.01)\n",
    "\n",
    "        return q_eval.mean().item()\n",
    "\n",
    "def main():\n",
    "    env = gym.make('Pendulum-v1')\n",
    "    agent = DQNAgent()\n",
    "\n",
    "    training_records = []\n",
    "    running_reward, running_q = -1000, 0\n",
    "    best_reward = -float('inf')\n",
    "\n",
    "    for i_ep in range(1000):\n",
    "        score = 0\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        for t in range(200):\n",
    "            action, action_index = agent.select_action(state)\n",
    "            state_, reward, terminated, truncated, _ = env.step(action)\n",
    "            score += reward\n",
    "            agent.store_transition(Transition(state, action_index, (reward + 8) / 8, state_))\n",
    "            state = state_\n",
    "            if agent.memory.isfull:\n",
    "                q = agent.update()\n",
    "                running_q = 0.99 * running_q + 0.01 * q\n",
    "\n",
    "        running_reward = running_reward * 0.9 + score * 0.1\n",
    "        training_records.append(TrainingRecord(i_ep, running_reward))\n",
    "\n",
    "        if i_ep % 20 == 0:\n",
    "            print('Ep {}\\tAverage score: {:.2f}\\tAverage Q: {:.2f}'.format(\n",
    "                i_ep, running_reward, running_q))\n",
    "\n",
    "#         if running_reward > best_reward:\n",
    "#             best_reward = running_reward\n",
    "#             agent.save_param()\n",
    "#             with open('./dqn_training_records.pkl', 'wb') as f:\n",
    "#                 pickle.dump(training_records, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "            \n",
    "#         if running_reward > -10:\n",
    "#             print(\"Solved! Running reward is now {}!\".format(running_reward))\n",
    "#             agent.save_params()\n",
    "#             with open('./dqn_training_records.pkl', 'wb') as f:\n",
    "#                 pickle.dump(training_records, f)\n",
    "#             break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    plt.plot([r.ep for r in training_records], [r.reward for r in training_records])\n",
    "    plt.title('DQN')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Moving averaged episode reward')\n",
    "    plt.savefig(\"./dqn.png\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
